{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: Planner Finetune and Evaluation Lab\n",
        "\n",
        "Audience:\n",
        "- Researchers running planner-only tuning experiments with fixed worker/verifier/generator endpoints.\n",
        "\n",
        "Learning goals:\n",
        "- Build experiment grids and launch commands reproducibly.\n",
        "- Parse rollout artifacts and summarize planning metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outline\n",
        "\n",
        "1. Define experiment matrix\n",
        "2. Generate training commands\n",
        "3. Parse rollout artifacts\n",
        "4. Compare checkpoints using planning-aware metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "REPO = Path(\"/Users/admin/TuanDung/repos/AgentFlow\")\n",
        "ROLLOUT_ROOT = REPO / \"rollout_data\"\n",
        "REPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experiment:\n",
        "    name: str\n",
        "    train_temp: float\n",
        "    max_steps: int\n",
        "    rollout_n: int\n",
        "\n",
        "grid = [\n",
        "    Experiment(name=f\"planner_t{t}_s{s}_r{r}\", train_temp=t, max_steps=s, rollout_n=r)\n",
        "    for t, s, r in itertools.product([0.3, 0.5, 0.7], [2, 3], [4, 8])\n",
        "]\n",
        "\n",
        "len(grid), grid[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_train_command(exp: Experiment) -> str:\n",
        "    # Planner-only tuning with fixed worker/verifier/generator is configured in model-engine routing.\n",
        "    # Adjust keys according to your `train/config.yaml` conventions.\n",
        "    return (\n",
        "        f\"cd {REPO} && python train/train_agent.py \"\n",
        "        f\"EXPERIMENT_NAME={exp.name} \"\n",
        "        f\"TRAIN_TEMPERATURE={exp.train_temp} \"\n",
        "        f\"TOOL_STEPS={exp.max_steps} \"\n",
        "        f\"actor_rollout_ref.rollout.n={exp.rollout_n}\"\n",
        "    )\n",
        "\n",
        "commands = [build_train_command(exp) for exp in grid[:5]]\n",
        "commands\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step - Parse rollout artifacts\n",
        "\n",
        "This parser reads rollout JSON files and computes lightweight planning metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iter_rollout_files(root: Path):\n",
        "    if not root.exists():\n",
        "        return []\n",
        "    return list(root.rglob(\"rollout_*.json\"))\n",
        "\n",
        "def summarize_rollouts(files):\n",
        "    rows = []\n",
        "    for fp in files:\n",
        "        try:\n",
        "            data = json.loads(fp.read_text())\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        reward = float(data.get(\"reward\", 0.0))\n",
        "        total_result = data.get(\"total_result\", {})\n",
        "        step_count = int(total_result.get(\"step_count\", 0)) if isinstance(total_result, dict) else 0\n",
        "\n",
        "        rows.append({\n",
        "            \"file\": str(fp),\n",
        "            \"reward\": reward,\n",
        "            \"step_count\": step_count,\n",
        "        })\n",
        "\n",
        "    if not rows:\n",
        "        return {\"n\": 0, \"mean_reward\": 0.0, \"mean_step_count\": 0.0}\n",
        "\n",
        "    mean_reward = sum(r[\"reward\"] for r in rows) / len(rows)\n",
        "    mean_steps = sum(r[\"step_count\"] for r in rows) / len(rows)\n",
        "    return {\"n\": len(rows), \"mean_reward\": round(mean_reward, 4), \"mean_step_count\": round(mean_steps, 4)}\n",
        "\n",
        "files = iter_rollout_files(ROLLOUT_ROOT)\n",
        "summary = summarize_rollouts(files)\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def planning_score(mean_reward: float, mean_step_count: float, alpha: float = 0.1) -> float:\n",
        "    # Example efficiency-adjusted score: reward penalized by step length.\n",
        "    return mean_reward - alpha * mean_step_count\n",
        "\n",
        "score = planning_score(summary[\"mean_reward\"], summary[\"mean_step_count\"])\n",
        "{\"summary\": summary, \"planning_score\": round(score, 4)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. Add tool-failure rate and verifier-stop quality into the summary function.\n",
        "2. Compare top-3 checkpoints by both `mean_reward` and `planning_score`.\n",
        "3. Export results to CSV and attach confidence intervals over seeds.\n",
        "\n",
        "Validation note:\n",
        "- This notebook does not run heavy finetuning directly; it builds launch commands and evaluation harnesses.\n",
        "- Run training in terminal/GPU cluster, then re-open this notebook for analysis.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
